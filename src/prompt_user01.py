from langchain_ollama import ChatOllama
from langchain.prompts import ChatPromptTemplate

# Initialize the Ollama local model
llm = ChatOllama(model="mistral", temperature=0.7)

# Define the chat prompt template
chat_template = ChatPromptTemplate.from_messages([
    ("system", "You are an assistant specialized in data analysis."),
    ("user", "How can I analyze {data_type} data using {library}?"),
])

# Get user input for data type and analysis library
data_type = input("Enter the type of data you want to analyze (e.g., financial, medical, marketing): ").strip()
library = input("Enter the library you want to use (e.g., pandas, numpy, SQL, scikit-learn): ").strip()

# Fill the prompt with user inputs
prompt_filled = chat_template.format(data_type=data_type, library=library)

# Generate response
response = llm.invoke(prompt_filled)

# Extract and format the response
if hasattr(response, "content"):
    response_text = response.content.strip()

    # Add Markdown formatting
    md_content = f"# 📊 {data_type.capitalize()} Data Analysis using {library.capitalize()}\n\n{response_text}\n\n---\n*Generated by AI*"

    # Print response in a nicely formatted way
    print("\n" + "=" * 50)
    print(f"📊 AI Response - {data_type.capitalize()} Data Analysis using {library.capitalize()} 📊")
    print("=" * 50 + "\n")
    print(response_text)
    print("\n" + "=" * 50)

    # Save response to a Markdown file
    md_filename = f"{data_type.lower()}_{library.lower()}_data_analysis_guide.md"
    with open(md_filename, "w", encoding="utf-8") as file:
        file.write(md_content)

    print(f"\n✅ The response has been saved to '{md_filename}'.")
else:
    print("⚠️ No valid response received from the AI model.")
