from langchain_ollama import ChatOllama  # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯
from langchain.prompts import ChatPromptTemplate

# Initialize the Ollama local model
llm = ChatOllama(model="mistral", temperature=0.7)

# Define the chat prompt template
chat_template = ChatPromptTemplate.from_messages([
    ("system", "You are an assistant specialized in data analysis."),
    ("user", "How can I analyze {data_type} data using Pandas?"),
])

# Fill the prompt with a specific data type
data_type = "financial"
prompt_filled = chat_template.format(data_type=data_type)

# Generate response
response = llm.invoke(prompt_filled)

# Extract and format the response
if hasattr(response, "content"):
    response_text = response.content.strip()

    # Add Markdown formatting
    md_content = f"# ğŸ“Š Financial Data Analysis using Pandas\n\n{response_text}\n\n---\n*Generated by AI*"

    # Print response in a nicely formatted way
    print("\n" + "=" * 50)
    print("ğŸ“Š AI Response - Financial Data Analysis using Pandas ğŸ“Š")
    print("=" * 50 + "\n")
    print(response_text)
    print("\n" + "=" * 50)

    # Save response to a Markdown file
    md_filename = "financial_data_analysis_guide.md"
    with open(md_filename, "w", encoding="utf-8") as file:
        file.write(md_content)

    print(f"\nâœ… The response has been saved to '{md_filename}'.")
else:
    print("âš ï¸ No valid response received from the AI model.")
